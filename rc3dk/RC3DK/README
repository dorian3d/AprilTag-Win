Introduction
============

3DK offers real-time access to data from RealityCap's sensor fusion algorithms. The primary interface is provided by the RCSensorFusion class; your apps must also implement the RCSensorFusionDelegate protocol.

Requirements
============

* Your projects must be linked against the libc++ C++ standard library. To do this add "-lc++" to "Other linker flags" in your project file. If you use another library that require libstdc++ (such as opencv), it's ok to link against both (-lc++ -lstdc++).

* 3DK requires at least a dual-core A5 CPU, and of course, a camera, accelerometer, and gyroscope. Currently supported devices are iPhone 4S, 5, 5C, 5S, iPad 2, 3, 4, Mini, iPod Touch 5.

* The quality of your data may suffer when running a Debug build, especially on older devices, due to increased overhead and less consistent data delivery from the OS. To get more satisfactory output, use an optimized build, either by changing the optimization settings in your project (this may make debugging more difficult), or choosing a Profile build.

* If you choose to manage the device's sensors yourself, rather than using the classes we provide (AVSessionManager, LocationManager, MotionManager, VideoManager, and/or RCSensorDelegate), you must use the following settings:

    * 640x480 video @24-30fps - [AVCaptureSession setSessionPreset:AVCaptureSessionPreset640x480]
    * Back camera - [AVCaptureDevice position] == AVCaptureDevicePositionBack
    * 100Hz accelerometer and gyro updates - [CMMotionManager setAccelerometerUpdateInterval:.01], [cmMotionManager setGyroUpdateInterval:.01]

Getting started
===============

1. Build and run the sample application RC3DKSampleGL. Make sure that you enter your license key in LicenseHelper.h. When you run it, you'll go through a brief calibration process, then see an interface visualizing the motion of your device and the real time 3D point cloud (gridlines are in meters).

2. To see how to start building your own app, look at RC3DKSampleApp. It is a simpler example that shows the bare minimum needed to get an app working with 3DK.

3. Have fun!

Known limitations and issues
============================

* We are working to allow you to configure how much CPU 3DK should use, as well as provide more options for camera configurations (see the Requirements section above).

* If your app tries to use 3DK without providing location and implementing a calibration procedure (as in the sample app), you may receive errors from the sensor fusion system and/or poor results. In some cases these results may improve over time as RCSensorFusion calibrates itself internally. Additionally, even with a properly implemented calibration procedure, output does improve over time. We are working to improve the robustness and consistency of results under these conditions.

* 3DK relies on computer vision to function. Low light conditions, covering the camera, reflections, or looking at a blank wall can all cause poor results and/or errors. 3DK uses the back camera on the device. Make sure that the back camera is not blocked by your finger, a protective case, etc.

Example Code
============

 Typical usage of this library would go something like the following. Note that there is a one-time calibration procedure that is
 not shown here. The one-time calibration procedure is optional, but highly recommended. The system incrementally calibrates
 itself after each run, but doing the calibration procedure before the first run will ensure that you get good results in the first
 few runs, before the system can automatically calibrate itself. See the sample app to see how the one-time calibration procedure 
 is done.

    // Get the sensor fusion object and set a delegate that 
    // implements the RCSensorFusionDelegate protocol.
    RCSensorFusion* sensorFusion = [RCSensorFusion sharedInstance];
    sensorFusion.delegate = self;

    // Set the license key. This must be done before starting sensor
    // fusion for the first time.
    [sensorFusion setLicenseKey:apiKey];

    // Pass in a CLLocation object that represents the device's 
    // current location.
    [sensorFusion setLocation:location];

    // Start sensor fusion,
    // which results in 6DOF device motion and point cloud output.
    // Pass in the AVCaptureDevice to be used for video capture.
    [sensorFusion startSensorFusionWithDevice:backCamera];

    // Begin passing in video frames and inertial data.
    [sensorFusion receiveVideoFrame:sampleBuffer];
    [sensorFusion receiveAccelerometerData:accelerometerData];
    [sensorFusion receiveGyroData:gyroData];

    // Implement the RCSensorFusionDelegate protocol methods to receive 
    // sensor fusion data.
    - (void) sensorFusionDidUpdateData:(RCSensorFusionData*)data {}
    - (void) sensorFusionDidChangeStatus:(RCSensorFusionStatus*)status {}

    // When you no longer want to receive sensor fusion data, stop it.
    [sensorFusion stopSensorFusion];
